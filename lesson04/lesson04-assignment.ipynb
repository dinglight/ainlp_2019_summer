{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于维基百科的词向量构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-01: Download Wikipedia Chinese Corpus: https://dumps.wikimedia.org/zhwiki/20190720/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "wget https://dumps.wikimedia.org/zhwiki/20190720/zhwiki-20190720-pages-articles-multistream.xml.bz2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-02: Using https://github.com/attardi/wikiextractor to extract the wikipedia corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "git clone https://github.com/attardi/wikiextractor.git\n",
    "cd wikiextracter\n",
    "python WikiExtractor.py -o zhwiki zhwiki-20190720-pages-articles-multistream.xml.bz2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-03: Using gensim get word vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用jieba分词，存储为文件corpus_zhwiki.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "def preprocess_file(in_file, fout):\n",
    "    with open(in_file, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            sline = line.strip()\n",
    "            # remove empty line\n",
    "            if sline == \"\":\n",
    "                continue\n",
    "            # remove html mark line\n",
    "            pattern = re.compile('<.*?>')\n",
    "            if pattern.match(sline):\n",
    "                continue\n",
    "            # jieba\n",
    "            seg_list = jieba.cut(sline)\n",
    "            seg_res = ' '.join(seg_list)\n",
    "            fout.write(seg_res)\n",
    "            fout.write('\\n')\n",
    "\n",
    "def preprocess(in_folder, out_file):\n",
    "    fout = open(out_file, 'a', encoding='utf-8')\n",
    "    for root, dirs, files in os.walk(in_folder):\n",
    "        for file_name in files:\n",
    "            in_file = os.path.join(root,file_name)\n",
    "            preprocess_file(in_file, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "preprocess('zhwiki', 'corpus_zhwiki.txt')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing\n",
    "\n",
    "def word2vec_train(in_file, out_model, out_vector):\n",
    "    sentences = LineSentence(in_file)\n",
    "    model = Word2Vec(sentences, workers=multiprocessing.cpu_count())\n",
    "    model.save(out_model)\n",
    "    model.wv.save_word2vec_format(out_vector, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "word2vec_train('corpus_zhwiki.txt', 'zhwiki.word2vec.model', 'zhwiki.word2vec.vectors')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-04: 使用词向量模型测试同义词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "zh_wiki_word2vec_model = Word2Vec.load('zhwiki.word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('足球运动', 0.7593598365783691),\n",
       " ('排球', 0.7379463911056519),\n",
       " ('冰球', 0.7345577478408813),\n",
       " ('手球', 0.7277976274490356),\n",
       " ('足球联赛', 0.7249800562858582),\n",
       " ('板球', 0.7136474847793579),\n",
       " ('橄欖球隊', 0.6775943040847778),\n",
       " ('橄欖球', 0.6566696166992188),\n",
       " ('踢球', 0.656656801700592),\n",
       " ('德甲球', 0.6550092697143555)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_wiki_word2vec_model.wv.most_similar(u\"足球\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('天津', 0.8689776659011841),\n",
       " ('北京', 0.8609808683395386),\n",
       " ('南京', 0.8212747573852539),\n",
       " ('杭州', 0.8176919221878052),\n",
       " ('广州', 0.7898254990577698),\n",
       " ('上海市', 0.7564722895622253),\n",
       " ('武汉', 0.7561065554618835),\n",
       " ('北平', 0.7394840717315674),\n",
       " ('重庆', 0.729672908782959),\n",
       " ('成都', 0.71846604347229)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_wiki_word2vec_model.wv.most_similar(u\"上海\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-05: 词向量可视化\n",
    "\n",
    "https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "tsne_plot(zh_wiki_word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

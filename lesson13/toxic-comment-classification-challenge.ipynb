{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU+FastText 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 30000\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_texts = train['comment_text'].fillna('').values\n",
    "X_train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 读取测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_texts = test['comment_text'].fillna('').values\n",
    "X_test_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 建立token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "token.fit_on_texts(list(X_train_texts)+list(X_test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 将“评论文字”转换为“数字列表”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seqs = token.texts_to_sequences(X_train_texts)\n",
    "X_test_seqs  = token.texts_to_sequences(X_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[733,\n",
       " 78,\n",
       " 1,\n",
       " 140,\n",
       " 131,\n",
       " 182,\n",
       " 30,\n",
       " 712,\n",
       " 4438,\n",
       " 10284,\n",
       " 1252,\n",
       " 86,\n",
       " 368,\n",
       " 51,\n",
       " 2230,\n",
       " 14039,\n",
       " 49,\n",
       " 6744,\n",
       " 15,\n",
       " 60,\n",
       " 2624,\n",
       " 151,\n",
       " 7,\n",
       " 2832,\n",
       " 33,\n",
       " 115,\n",
       " 1246,\n",
       " 16129,\n",
       " 2517,\n",
       " 5,\n",
       " 50,\n",
       " 59,\n",
       " 256,\n",
       " 1,\n",
       " 370,\n",
       " 31,\n",
       " 1,\n",
       " 46,\n",
       " 29,\n",
       " 144,\n",
       " 72,\n",
       " 3931,\n",
       " 89,\n",
       " 4208,\n",
       " 6368,\n",
       " 2687,\n",
       " 1183]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 截长补短让所有“数字列表”的长度都为100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train_seqs, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "X_test  = sequence.pad_sequences(X_test_seqs, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,   733,\n",
       "          78,     1,   140,   131,   182,    30,   712,  4438, 10284,\n",
       "        1252,    86,   368,    51,  2230, 14039,    49,  6744,    15,\n",
       "          60,  2624,   151,     7,  2832,    33,   115,  1246, 16129,\n",
       "        2517,     5,    50,    59,   256,     1,   370,    31,     1,\n",
       "          46,    29,   144,    72,  3931,    89,  4208,  6368,  2687,\n",
       "        1183])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 建立嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394787 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = token.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 1\n",
      "to 2\n",
      "of 3\n",
      "a 4\n",
      "and 5\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for (k,v) in word_index.items():\n",
    "    if count == 5:break\n",
    "    print(k, v)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 单词到词向量的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_vectors(fname):\n",
    "    file = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    vocab_and_vectors = {}\n",
    "    # put words as dict indexes and vectors as words values\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        vocab_and_vectors[word] = vector\n",
    "    return vocab_and_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_vectors('../../data/crawl-300d-2M.vec')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found {} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.300e-01,  1.817e-01, -2.394e-01, -3.343e-01, -1.782e-01,\n",
       "        1.909e-01,  2.360e-01, -3.340e-02, -2.144e-01,  2.300e-03,\n",
       "        2.884e-01,  1.728e-01,  9.580e-02, -1.631e-01, -1.694e-01,\n",
       "        1.399e-01, -3.348e-01,  8.230e-02, -2.362e-01,  1.736e-01,\n",
       "       -1.211e-01, -1.921e-01,  2.112e-01, -3.219e-01,  1.304e-01,\n",
       "        6.290e-02,  2.469e-01,  2.399e-01,  3.390e-02, -2.660e-02,\n",
       "       -2.490e-01,  1.621e-01, -2.325e-01, -6.460e-02, -5.870e-02,\n",
       "        2.026e-01,  3.257e-01,  1.077e-01,  1.626e-01,  3.746e-01,\n",
       "       -4.290e-02, -1.358e-01, -8.100e-02,  1.301e-01, -2.484e-01,\n",
       "       -1.489e-01,  1.837e-01, -2.005e-01, -3.705e-01, -1.698e-01,\n",
       "       -4.474e-01,  1.324e-01,  1.508e-01,  2.100e-03,  1.410e-02,\n",
       "       -8.110e-02, -3.260e-02, -3.553e-01, -1.793e-01, -1.990e-02,\n",
       "       -2.126e-01,  1.082e-01, -6.460e-02, -4.052e-01,  3.202e-01,\n",
       "        1.000e-02, -9.220e-02, -2.408e-01, -4.420e-02, -4.280e-01,\n",
       "        2.139e-01, -3.097e-01, -1.249e-01,  2.535e-01,  2.053e-01,\n",
       "        3.272e-01, -3.369e-01, -1.598e-01,  2.050e-01, -3.159e-01,\n",
       "        5.960e-02,  5.140e-02,  6.270e-02, -2.369e-01,  4.490e-01,\n",
       "       -1.726e-01, -9.630e-02,  8.180e-02,  1.752e-01,  1.541e-01,\n",
       "        1.635e-01,  2.828e-01, -7.400e-03,  2.295e-01,  4.140e-02,\n",
       "        7.300e-03,  3.208e-01,  2.272e-01, -5.119e-01,  8.240e-02,\n",
       "       -1.260e-02,  4.160e-02,  2.252e-01, -6.150e-02,  7.490e-02,\n",
       "        1.110e-02,  1.386e-01, -1.816e-01,  0.000e+00, -2.304e-01,\n",
       "        8.880e-02, -1.115e-01,  1.202e-01,  1.509e-01, -2.468e-01,\n",
       "        1.312e-01, -2.489e-01,  2.658e-01,  1.661e-01,  8.730e-02,\n",
       "       -2.118e-01,  4.040e-02, -4.473e-01,  1.372e-01, -9.120e-02,\n",
       "        1.099e-01,  1.926e-01,  2.138e-01, -5.440e-02,  5.900e-02,\n",
       "        8.500e-02, -6.620e-02, -1.651e-01, -6.720e-02,  3.480e-02,\n",
       "       -1.153e-01, -4.000e-04,  2.050e-02, -3.960e-02, -2.181e-01,\n",
       "        3.463e-01,  2.128e-01,  1.512e-01,  4.172e-01, -1.513e-01,\n",
       "        4.570e-02,  8.880e-02, -1.139e-01, -8.180e-02,  1.166e-01,\n",
       "        2.061e-01, -1.145e-01,  2.120e-01,  2.776e-01, -1.051e-01,\n",
       "        9.090e-02, -1.480e-02,  1.124e-01,  5.540e-02,  5.420e-02,\n",
       "        6.875e-01, -8.680e-02,  8.580e-02,  3.100e-03,  6.460e-02,\n",
       "        7.560e-02,  1.019e-01, -9.700e-03,  2.116e-01,  2.255e-01,\n",
       "        7.870e-02, -1.065e-01,  9.460e-02, -4.131e-01, -1.825e-01,\n",
       "        3.697e-01, -1.600e-01,  1.691e-01,  4.018e-01, -4.000e-03,\n",
       "       -1.418e-01, -2.351e-01, -2.143e-01,  1.617e-01, -2.726e-01,\n",
       "        5.620e-02, -2.600e-03,  3.400e-02,  3.184e-01,  2.915e-01,\n",
       "        2.067e-01,  3.336e-01, -2.136e-01,  1.576e-01,  3.238e-01,\n",
       "        1.294e-01,  1.306e-01, -6.690e-02, -1.887e-01, -2.660e-02,\n",
       "        2.574e-01, -3.235e-01,  9.400e-02, -8.940e-02,  6.980e-02,\n",
       "        4.090e-02, -3.490e-02, -1.520e-02,  1.602e-01, -1.583e-01,\n",
       "        3.346e-01,  6.633e-01,  7.670e-02, -2.654e-01,  1.852e-01,\n",
       "       -4.199e-01,  4.155e-01, -3.560e-02, -2.918e-01, -8.440e-02,\n",
       "       -9.790e-02,  3.599e-01,  7.030e-02, -2.400e-02,  9.330e-02,\n",
       "       -4.970e-02, -3.013e-01, -6.530e-02, -1.535e-01, -3.202e-01,\n",
       "       -4.563e-01, -3.285e-01, -1.040e-01, -1.735e-01, -1.000e-02,\n",
       "       -5.023e-01, -2.610e-01,  1.723e-01, -3.550e-02,  3.680e-01,\n",
       "       -3.306e-01,  8.790e-02, -4.887e-01,  1.207e-01,  2.390e-02,\n",
       "        8.130e-02,  3.149e-01,  7.220e-02, -6.770e-02, -1.526e-01,\n",
       "       -1.020e-01,  2.720e-01, -1.415e-01,  1.768e-01, -8.930e-02,\n",
       "       -8.500e-02,  3.780e-02,  5.730e-02, -2.261e-01,  1.364e-01,\n",
       "        1.717e-01, -2.160e-02,  7.850e-02, -3.259e-01,  9.530e-02,\n",
       "        3.750e-01,  1.460e-02,  2.870e-02, -1.298e-01,  1.751e-01,\n",
       "       -1.599e-01, -1.627e-01, -1.507e-01,  2.640e-01, -1.248e-01,\n",
       "        1.830e-02,  5.230e-02, -8.430e-02, -1.624e-01,  1.800e-03,\n",
       "       -3.718e-01,  2.240e-02, -7.093e-01,  2.675e-01, -1.196e-01,\n",
       "       -2.529e-01, -5.060e-02,  2.518e-01,  6.230e-02, -2.240e-02,\n",
       "       -2.093e-01, -3.340e-02, -8.470e-02, -5.590e-02, -2.130e-02,\n",
       "        2.126e-01,  8.160e-02, -6.940e-02, -2.443e-01,  2.560e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 单词索引到词向量的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.06299996e-01,  6.69000000e-02,  6.37999997e-02,  3.28900009e-01,\n",
       "        1.37999998e-02, -3.37000005e-02, -1.95899993e-01, -4.25999999e-01,\n",
       "        7.24900007e-01, -5.04000001e-02, -6.88000023e-02,  7.53000006e-02,\n",
       "       -7.86999986e-02, -4.23999988e-02,  3.37999985e-02, -4.00000019e-03,\n",
       "        2.50900000e-01,  7.80000016e-02,  1.17500000e-01,  3.51000018e-02,\n",
       "       -2.39700004e-01, -3.92999984e-02,  2.34999992e-02, -7.66000003e-02,\n",
       "        2.00000009e-03, -5.13000004e-02, -1.24600001e-01,  9.61000025e-02,\n",
       "       -6.45999983e-02, -1.26599997e-01, -1.28399998e-01,  2.61000004e-02,\n",
       "       -2.44999994e-02, -1.48599997e-01,  1.29000004e-02, -1.02000004e-02,\n",
       "        9.83000025e-02, -1.20800003e-01,  7.46000037e-02,  1.04099996e-01,\n",
       "       -9.42000002e-02,  2.24199995e-01, -1.54300004e-01, -3.11999992e-02,\n",
       "        7.51999989e-02,  2.85000000e-02, -4.08999994e-02,  4.36000004e-02,\n",
       "        5.51999994e-02, -2.31000006e-01, -3.40000018e-02, -7.91100025e-01,\n",
       "        8.62200022e-01,  5.18999994e-02, -2.03500003e-01,  4.12999988e-02,\n",
       "       -7.06000030e-02,  5.26700020e-01, -1.84599996e-01,  1.39200002e-01,\n",
       "       -1.39899999e-01,  1.06700003e-01,  1.75999999e-02,  1.66999996e-02,\n",
       "       -5.22000007e-02,  2.65599996e-01, -3.03000007e-02, -5.75999990e-02,\n",
       "       -1.71000008e-02, -9.39999968e-02,  1.79700002e-01,  3.33999991e-02,\n",
       "       -6.36000037e-02,  2.42599994e-01,  3.79999988e-02, -4.17800009e-01,\n",
       "        4.91400003e-01, -4.80999984e-02,  3.69500011e-01,  5.27000017e-02,\n",
       "        6.81800008e-01,  1.12899996e-01,  3.39000002e-02, -8.56000036e-02,\n",
       "        8.94000009e-02, -1.08000003e-02,  9.31999981e-02,  7.97000006e-02,\n",
       "       -1.83799997e-01, -4.49999981e-03,  6.58000037e-02, -1.51500002e-01,\n",
       "        7.81999975e-02, -7.69999996e-03,  1.87099993e-01, -1.60799995e-01,\n",
       "       -5.09299994e-01,  4.96999994e-02, -5.16000018e-02,  1.04699999e-01,\n",
       "       -4.10000011e-02,  8.60000029e-03, -6.69000000e-02, -2.51000002e-02,\n",
       "        4.17999998e-02, -2.06100002e-01,  8.59000012e-02, -5.62999994e-02,\n",
       "       -6.96000010e-02,  5.05000018e-02, -9.89999995e-03, -1.11299999e-01,\n",
       "        2.56999992e-02,  6.45999983e-02, -8.14800024e-01, -3.95000018e-02,\n",
       "       -5.88000007e-02, -7.33999982e-02,  4.58999984e-02, -1.44299999e-01,\n",
       "        3.29999998e-02, -6.08000010e-02,  2.69400001e-01,  4.58999984e-02,\n",
       "       -1.90899998e-01,  7.91999996e-02,  4.14999984e-02, -2.04600006e-01,\n",
       "        3.13199997e-01, -1.97400004e-01, -1.63000003e-02, -3.66000012e-02,\n",
       "        3.73000018e-02,  2.70000007e-03, -6.78900003e-01, -1.41700000e-01,\n",
       "       -2.64800012e-01, -4.38000001e-02,  2.65899986e-01,  4.55799997e-01,\n",
       "       -1.16099998e-01, -7.75000006e-02,  2.89999996e-03,  1.42700002e-01,\n",
       "        5.16000018e-02, -1.21699996e-01, -1.30000000e-03,  8.69999975e-02,\n",
       "       -5.31000011e-02, -7.75000006e-02, -1.58399999e-01,  4.06999998e-02,\n",
       "        9.49999988e-02, -4.82000001e-02,  7.32000023e-02,  5.20000001e-03,\n",
       "        1.84400007e-01,  7.44000003e-02,  1.14100002e-01, -6.89999983e-02,\n",
       "        9.14300025e-01,  1.04599997e-01, -1.40000004e-02, -1.65500000e-01,\n",
       "        1.96199998e-01, -1.13000004e-02, -6.61000013e-02, -7.63999969e-02,\n",
       "        1.03900000e-01, -2.57600009e-01,  2.40000002e-02,  1.07400000e-01,\n",
       "       -1.07299998e-01, -1.58199996e-01,  2.53699988e-01,  2.01999992e-02,\n",
       "       -1.76400006e-01, -1.83799997e-01, -5.35000004e-02, -3.31000015e-02,\n",
       "        5.93900025e-01, -6.80000009e-03, -8.16000029e-02, -7.84000009e-02,\n",
       "        1.55800000e-01, -7.63999969e-02, -5.90999983e-02, -2.07699999e-01,\n",
       "       -3.61299992e-01, -4.63000014e-02,  1.30300000e-01,  1.25000002e-02,\n",
       "       -9.96999964e-02, -4.23999988e-02,  1.96000002e-02,  3.64000015e-02,\n",
       "       -8.28000009e-02,  5.53000011e-02,  6.49999976e-02,  4.49999981e-03,\n",
       "        1.10500000e-01, -6.97999969e-02, -1.92000009e-02, -6.61000013e-02,\n",
       "       -4.94000018e-02,  7.38999993e-02, -7.90000036e-02, -2.63000000e-02,\n",
       "       -3.15000005e-02, -3.32500011e-01,  4.89000008e-02,  8.69499981e-01,\n",
       "        8.91999975e-02, -5.64999990e-02,  7.54000023e-02, -2.95000002e-02,\n",
       "        2.19000001e-02,  4.49000001e-02,  7.50000030e-02,  6.81999996e-02,\n",
       "       -1.53300002e-01,  2.06799999e-01,  6.99999975e-04, -3.37000005e-02,\n",
       "       -1.09300002e-01,  5.60999997e-02,  2.87999995e-02,  9.87000018e-02,\n",
       "       -1.33800000e-01, -3.90999988e-02, -7.66000003e-02,  6.87000006e-02,\n",
       "       -4.21999991e-02,  2.52000000e-02, -2.77999993e-02,  2.44800001e-01,\n",
       "        1.75300002e-01, -1.31999999e-02, -6.35000020e-02, -2.77999993e-02,\n",
       "       -1.57999992e-02, -6.79000020e-02, -2.11600006e-01, -1.66299999e-01,\n",
       "        1.35800004e-01,  3.83000001e-02,  2.75500000e-01,  8.73999968e-02,\n",
       "       -3.13000008e-02,  2.70000007e-03, -8.20000004e-03, -2.05699995e-01,\n",
       "       -1.53999999e-02, -4.91000004e-02, -2.21900001e-01, -1.29299998e-01,\n",
       "        5.60000017e-02,  1.64999999e-02, -5.57000004e-02,  2.60000001e-03,\n",
       "       -9.66999978e-02,  2.64999997e-02, -3.31999995e-02,  4.06999998e-02,\n",
       "       -4.69000004e-02,  2.35900000e-01, -1.60000008e-02,  1.45899996e-01,\n",
       "       -1.65299997e-01, -3.53999995e-02,  9.22999978e-02,  4.88999993e-01,\n",
       "        1.56000003e-01, -8.98000002e-02, -4.30000015e-02, -8.48999992e-02,\n",
       "       -1.29199997e-01,  3.22999991e-02, -4.83000018e-02,  1.72099993e-01,\n",
       "       -1.23400003e-01,  2.65100002e-01, -2.50499994e-01, -5.20000001e-03,\n",
       "       -3.95000018e-02,  1.97600007e-01, -4.30000015e-03,  9.99999975e-05,\n",
       "       -1.03000000e-01, -1.55100003e-01, -8.28000009e-02,  6.56000003e-02,\n",
       "        1.30899996e-01,  3.73899996e-01, -5.51000014e-02, -4.13300008e-01,\n",
       "       -1.68999992e-02, -1.82899997e-01,  1.35199994e-01,  4.19999994e-02])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 建立嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 建立GRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Program\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#model.add(Embedding(input_dim = MAX_NUM_WORDS,\n",
    "#                    output_dim = EMBEDDING_DIM,\n",
    "#                    input_length=MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim = num_words,\n",
    "                    output_dim = EMBEDDING_DIM,\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(GRU(units=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=6, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          9000000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 300)               540900    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1806      \n",
      "=================================================================\n",
      "Total params: 9,542,706\n",
      "Trainable params: 542,706\n",
      "Non-trainable params: 9,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',  \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/2\n",
      " - 1907s - loss: 0.0518 - acc: 0.9813 - val_loss: 0.0447 - val_acc: 0.9832\n",
      "Epoch 2/2\n",
      " - 1882s - loss: 0.0428 - acc: 0.9836 - val_loss: 0.0431 - val_acc: 0.9835\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(X_train, y_train,batch_size=32, epochs=2, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./data/sample_submission.csv')\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

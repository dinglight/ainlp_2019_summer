{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import jieba\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/comment-classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+'sentiment_analysis_trainingset.csv', nrows=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(text):\n",
    "    return ' '.join(jieba.cut(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DINGLI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.058 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "content = content.apply(cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embeddings(fname):\n",
    "    vocab_and_vectors = {}\n",
    "    with open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore') as file:\n",
    "        # put words as dict indexes and vectors as words values\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            vocab_and_vectors[word] = vector\n",
    "    return vocab_and_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = load_word_embeddings('../../data/fasttext/cc.zh.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequence(texts, max_encoder_seq_length, word_embeddings):\n",
    "    embedding_size = word_embeddings['测试'].shape[0]\n",
    "    results = np.zeros((len(texts), max_encoder_seq_length, embedding_size), dtype='float32')\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        words = text.split()\n",
    "        words = [w for w in words if w in word_embeddings]\n",
    "        seq_length = len(words)\n",
    "        padding_size = (max_encoder_seq_length - seq_length) if (max_encoder_seq_length > seq_length) else 0\n",
    "        for j in range(max_encoder_seq_length):\n",
    "            if j < padding_size:\n",
    "                results[i][j] = np.zeros_like(word_embeddings['测试'])\n",
    "            else:\n",
    "                results[i][j] = word_embeddings[words[j-padding_size]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequence(content, 256,word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 256, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理目标数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = df[columns[2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape (1000, 20, 4)\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(train_labels+2, num_classes=4)\n",
    "print(\"y_train shape {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_train = np.vstack([np.zeros((1,20,4), dtype='float32'), y_train[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立seq2seq模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_input, n_output, n_units):\n",
    "    '''\n",
    "    n_input: word vec size\n",
    "    n_output: class numbers\n",
    "    n_units: The number of cells to create in the encoder and decoder models, e.g. 128 or 256.\n",
    "    '''\n",
    "    # define training encoder\n",
    "    encoder_inputs = Input(shape=(None, n_input))\n",
    "    encoder = LSTM(n_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    # define training decoder\n",
    "    decoder_inputs = Input(shape=(None, n_output))\n",
    "    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    # define inference encoder\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    # define inference decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    # return all models\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "num_encoder_tokens = 300  # 300 维的词向量\n",
    "num_decoder_tokens = 4    # 4 种情感等级\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "encoder_input_data = x_train\n",
    "decoder_input_data = x2_train\n",
    "decoder_target_data = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 34s 34ms/step - loss: 1.0533 - acc: 0.5933\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 29s 29ms/step - loss: 0.9561 - acc: 0.6186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23424835780>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "train, infenc, infdec = define_models(num_encoder_tokens, num_decoder_tokens, latent_dim)\n",
    "train.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# train model\n",
    "train.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(infenc, infdec, source, n_steps, cardinality):\n",
    "    '''\n",
    "    infenc: Encoder model used when making a prediction for a new source sequence.\n",
    "    infdec: Decoder model use when making a prediction for a new source sequence.\n",
    "    source: Encoded source sequence.\n",
    "    n_steps: Number of time steps in the target sequence.\n",
    "    cardinality: The cardinality of the output sequence, e.g. the number of features, words, or characters for each time step.\n",
    "    '''\n",
    "    # encode\n",
    "    state = infenc.predict(source)\n",
    "    # start of sequence input\n",
    "    target_seq = np.array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "    for t in range(n_steps):\n",
    "        # predict next char\n",
    "        yhat, h, c = infdec.predict([target_seq] + state)\n",
    "        # store prediction\n",
    "        output.append(yhat[0,0,:])\n",
    "        # update state\n",
    "        state = [h, c]\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict_sequence(infenc, infdec, x_train[0:1], 20, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77009726, 0.02088405, 0.01089752, 0.19812119],\n",
       "       [0.78496355, 0.0287317 , 0.01616323, 0.17014155],\n",
       "       [0.78814805, 0.03786965, 0.02382274, 0.15015954],\n",
       "       [0.78157836, 0.04684557, 0.03336274, 0.13821335],\n",
       "       [0.7688455 , 0.05472791, 0.04410048, 0.13232611],\n",
       "       [0.7524493 , 0.0607225 , 0.05531617, 0.13151205],\n",
       "       [0.73344266, 0.06421298, 0.06628405, 0.13606033],\n",
       "       [0.7123251 , 0.06535235, 0.07643841, 0.14588408],\n",
       "       [0.6895445 , 0.0648436 , 0.08538803, 0.16022395],\n",
       "       [0.66568786, 0.06341065, 0.09286465, 0.17803682],\n",
       "       [0.64149517, 0.06157431, 0.09875781, 0.1981727 ],\n",
       "       [0.617762  , 0.05964941, 0.10313462, 0.21945389],\n",
       "       [0.59521884, 0.05780599, 0.106194  , 0.2407812 ],\n",
       "       [0.57443714, 0.05612673, 0.108195  , 0.2612411 ],\n",
       "       [0.5557863 , 0.05464415, 0.1093986 , 0.28017092],\n",
       "       [0.53943443, 0.05336286, 0.11003404, 0.29716864],\n",
       "       [0.52538323, 0.05227229, 0.11028495, 0.31205955],\n",
       "       [0.51351434, 0.05135427, 0.11028925, 0.32484215],\n",
       "       [0.5036345 , 0.05058744, 0.11014524, 0.33563283],\n",
       "       [0.49551377, 0.04995025, 0.10991996, 0.3446161 ]], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decode(encoded_seq):\n",
    "    result = np.array([np.argmax(vector) for vector in encoded_seq], dtype=int)\n",
    "    result = result -2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_decode = one_hot_decode(y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2,\n",
       "       -2, -2, -2], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred_decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(path+'sentiment_analysis_testa.csv', nrows=100)\n",
    "data_test = df_test['content']\n",
    "data_test = data_test.apply(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = vectorize_sequence(data_test, 256, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def load_train_model(model_name, n_units):\n",
    "    model = load_model(model_name)\n",
    "    encoder_inputs = model.input[0] # input_1\n",
    "    encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "    encoder_states = [state_h_enc, state_c_enc]\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_inputs = model.input[1] # input_2\n",
    "    decoder_state_input_h = Input(shape=(n_units,), name='input_3')\n",
    "    decoder_state_input_c = Input(shape=(n_units,), name='input_4')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_lstm = model.layers[3]\n",
    "    decoder_outputs, state_h_enc, state_c_enc = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "    decoder_states = [state_h_enc, state_c_enc]\n",
    "    decoder_dense = model.layers[4]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "infenc, infdec = load_train_model('./seq/s2s_train.h5', latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _ in enumerate(x_test):\n",
    "    encoded_seq = predict_sequence(infenc, infdec, x_test[i:i+1], 20, 4)\n",
    "    seq = one_hot_decode(encoded_seq)\n",
    "    df_test.iloc[i, 2:] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>location_traffic_convenience</th>\n",
       "      <th>location_distance_from_business_district</th>\n",
       "      <th>location_easy_to_find</th>\n",
       "      <th>service_wait_time</th>\n",
       "      <th>service_waiters_attitude</th>\n",
       "      <th>service_parking_convenience</th>\n",
       "      <th>service_serving_speed</th>\n",
       "      <th>price_level</th>\n",
       "      <th>...</th>\n",
       "      <th>environment_decoration</th>\n",
       "      <th>environment_noise</th>\n",
       "      <th>environment_space</th>\n",
       "      <th>environment_cleaness</th>\n",
       "      <th>dish_portion</th>\n",
       "      <th>dish_taste</th>\n",
       "      <th>dish_look</th>\n",
       "      <th>dish_recommendation</th>\n",
       "      <th>others_overall_experience</th>\n",
       "      <th>others_willing_to_consume_again</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"我想说他们家的优惠活动好持久啊，我预售的时候买的券，前两天心血来潮去吃的活动还在继续\\n首...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>\"终于开到心心念念的LAB loft。第一次来就随便点也一些～【香辣虾意面】蛮辣的，但其实一...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"地理位置好，交通方便，就在124车站对面交通方便，很好，我晚上7点多去买的了，已经没有什么...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>\"运气很好，抽中了大众点评的霸王餐。这家主题餐厅心仪已久了，种种原因一直未能成行，没想到抽中...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>\"幸运随点评团体验霸王餐，心情好~蜀九香刚进驻泉州不久，招牌大名气响，以至于刚到店门口的我被...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>\"先是办好了会员卡，因为用会员卡可以打8折买披萨，只限披萨，其他的不能打折。用会员卡买好披萨...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>\"环境看着挺整洁温馨的。里面有很多小楠很喜欢玩具和模型。一进去就和楠爹研究上了。我团购的，选...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>\"平心而论，这家店味道还OK，至少足够吸引武汉人民的新鲜感，然而对于这家店对深圳乐凯撒各方面...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>\"位于梅川路步行街 很好找 5点到的 不需要排队 坐下来把锅底和菜品点好 点的都是招牌 比如...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>\"整体给我感觉还不错的一家店，很久之前团的券啦，当时38.8代100现在涨到68代100啦。...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                            content  \\\n",
       "0    0  \"我想说他们家的优惠活动好持久啊，我预售的时候买的券，前两天心血来潮去吃的活动还在继续\\n首...   \n",
       "1    1  \"终于开到心心念念的LAB loft。第一次来就随便点也一些～【香辣虾意面】蛮辣的，但其实一...   \n",
       "2    2  \"地理位置好，交通方便，就在124车站对面交通方便，很好，我晚上7点多去买的了，已经没有什么...   \n",
       "3    3  \"运气很好，抽中了大众点评的霸王餐。这家主题餐厅心仪已久了，种种原因一直未能成行，没想到抽中...   \n",
       "4    4  \"幸运随点评团体验霸王餐，心情好~蜀九香刚进驻泉州不久，招牌大名气响，以至于刚到店门口的我被...   \n",
       "..  ..                                                ...   \n",
       "95  95  \"先是办好了会员卡，因为用会员卡可以打8折买披萨，只限披萨，其他的不能打折。用会员卡买好披萨...   \n",
       "96  96  \"环境看着挺整洁温馨的。里面有很多小楠很喜欢玩具和模型。一进去就和楠爹研究上了。我团购的，选...   \n",
       "97  97  \"平心而论，这家店味道还OK，至少足够吸引武汉人民的新鲜感，然而对于这家店对深圳乐凯撒各方面...   \n",
       "98  98  \"位于梅川路步行街 很好找 5点到的 不需要排队 坐下来把锅底和菜品点好 点的都是招牌 比如...   \n",
       "99  99  \"整体给我感觉还不错的一家店，很久之前团的券啦，当时38.8代100现在涨到68代100啦。...   \n",
       "\n",
       "    location_traffic_convenience  location_distance_from_business_district  \\\n",
       "0                           -2.0                                      -2.0   \n",
       "1                           -2.0                                      -2.0   \n",
       "2                           -2.0                                      -2.0   \n",
       "3                           -2.0                                      -2.0   \n",
       "4                           -2.0                                      -2.0   \n",
       "..                           ...                                       ...   \n",
       "95                          -2.0                                      -2.0   \n",
       "96                          -2.0                                      -2.0   \n",
       "97                          -2.0                                      -2.0   \n",
       "98                          -2.0                                      -2.0   \n",
       "99                          -2.0                                      -2.0   \n",
       "\n",
       "    location_easy_to_find  service_wait_time  service_waiters_attitude  \\\n",
       "0                    -2.0               -2.0                      -2.0   \n",
       "1                    -2.0               -2.0                      -2.0   \n",
       "2                    -2.0               -2.0                      -2.0   \n",
       "3                    -2.0               -2.0                      -2.0   \n",
       "4                    -2.0               -2.0                      -2.0   \n",
       "..                    ...                ...                       ...   \n",
       "95                   -2.0               -2.0                      -2.0   \n",
       "96                   -2.0               -2.0                      -2.0   \n",
       "97                   -2.0               -2.0                      -2.0   \n",
       "98                   -2.0               -2.0                      -2.0   \n",
       "99                   -2.0               -2.0                      -2.0   \n",
       "\n",
       "    service_parking_convenience  service_serving_speed  price_level  ...  \\\n",
       "0                          -2.0                   -2.0         -2.0  ...   \n",
       "1                          -2.0                   -2.0         -2.0  ...   \n",
       "2                          -2.0                   -2.0         -2.0  ...   \n",
       "3                          -2.0                   -2.0         -2.0  ...   \n",
       "4                          -2.0                   -2.0         -2.0  ...   \n",
       "..                          ...                    ...          ...  ...   \n",
       "95                         -2.0                   -2.0         -2.0  ...   \n",
       "96                         -2.0                   -2.0         -2.0  ...   \n",
       "97                         -2.0                   -2.0         -2.0  ...   \n",
       "98                         -2.0                   -2.0         -2.0  ...   \n",
       "99                         -2.0                   -2.0         -2.0  ...   \n",
       "\n",
       "    environment_decoration  environment_noise  environment_space  \\\n",
       "0                     -2.0               -2.0               -2.0   \n",
       "1                     -2.0               -2.0               -2.0   \n",
       "2                     -2.0               -2.0               -2.0   \n",
       "3                     -2.0               -2.0               -2.0   \n",
       "4                     -2.0               -2.0               -2.0   \n",
       "..                     ...                ...                ...   \n",
       "95                    -2.0               -2.0               -2.0   \n",
       "96                    -2.0               -2.0               -2.0   \n",
       "97                    -2.0               -2.0               -2.0   \n",
       "98                    -2.0               -2.0               -2.0   \n",
       "99                    -2.0               -2.0               -2.0   \n",
       "\n",
       "    environment_cleaness  dish_portion  dish_taste  dish_look  \\\n",
       "0                   -2.0          -2.0        -2.0       -2.0   \n",
       "1                   -2.0          -2.0        -2.0       -2.0   \n",
       "2                   -2.0          -2.0        -2.0       -2.0   \n",
       "3                   -2.0          -2.0        -2.0       -2.0   \n",
       "4                   -2.0          -2.0        -2.0       -2.0   \n",
       "..                   ...           ...         ...        ...   \n",
       "95                  -2.0          -2.0        -2.0       -2.0   \n",
       "96                  -2.0          -2.0        -2.0       -2.0   \n",
       "97                  -2.0          -2.0        -2.0       -2.0   \n",
       "98                  -2.0          -2.0        -2.0       -2.0   \n",
       "99                  -2.0          -2.0        -2.0       -2.0   \n",
       "\n",
       "    dish_recommendation  others_overall_experience  \\\n",
       "0                  -2.0                       -2.0   \n",
       "1                  -2.0                       -2.0   \n",
       "2                  -2.0                       -2.0   \n",
       "3                  -2.0                       -2.0   \n",
       "4                  -2.0                       -2.0   \n",
       "..                  ...                        ...   \n",
       "95                 -2.0                       -2.0   \n",
       "96                 -2.0                       -2.0   \n",
       "97                 -2.0                       -2.0   \n",
       "98                 -2.0                       -2.0   \n",
       "99                 -2.0                       -2.0   \n",
       "\n",
       "    others_willing_to_consume_again  \n",
       "0                              -2.0  \n",
       "1                              -2.0  \n",
       "2                              -2.0  \n",
       "3                              -2.0  \n",
       "4                              -2.0  \n",
       "..                              ...  \n",
       "95                             -2.0  \n",
       "96                             -2.0  \n",
       "97                             -2.0  \n",
       "98                             -2.0  \n",
       "99                             -2.0  \n",
       "\n",
       "[100 rows x 22 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

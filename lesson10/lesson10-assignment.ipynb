{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word2vec + fully connected neural networks to finish “⾖豆瓣评论” classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load douban movie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fname = 'movie_comments.csv'\n",
    "df = pd.read_csv(fname, encoding='utf8')\n",
    "df = df.fillna('')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261497, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "\n",
    "def preprocess(comments):\n",
    "    pattern = re.compile('[\\u4E00-\\u9FA5]')\n",
    "    new_comments = []\n",
    "    for comment in comments:\n",
    "        only_chinese = pattern.findall(comment)\n",
    "        chinese_comment = ''.join(only_chinese)\n",
    "        chinese_tokens = jieba.cut(chinese_comment)\n",
    "        new_comments.append(' '.join(chinese_tokens))\n",
    "    return new_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DINGLI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.085 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "comments = preprocess(df['comment'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf8 -*-\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import PSLvec as psl\n",
    "from nltk.tokenize import StanfordTokenizer\n",
    "\n",
    "word2vec_path = './GoogleNews-vectors-negative300.bin.gz'\n",
    "glove_path = './glove_model.txt'\n",
    "psl_path = './PSL_model.txt'\n",
    "# traindata = './datasets/sts2013.OnWN.pkl'\n",
    "freq_table = './mydictionary'\n",
    "embedding_size = 300\n",
    "\n",
    "pslemb = psl.PSL()\n",
    "\n",
    "# 载入word2vec模型\n",
    "# model = KeyedVectors.load_word2vec_format(word2vec_path,binary=True)\n",
    "# model = KeyedVectors.load_word2vec_format(glove_path,binary=False)\n",
    "# model = KeyedVectors.load_word2vec_format(psl_path,binary=False)\n",
    "model = pslemb.w\n",
    "print('完成模型载入')\n",
    "\n",
    "tokenizer = StanfordTokenizer(path_to_jar=r\"D:\\stanford-parser-full-2016-10-31\\stanford-parser.jar\")\n",
    "\n",
    "\n",
    "# print(type(model))\n",
    "# print(model['sdfsfsdfsadfs'])\n",
    "\n",
    "class Word:\n",
    "    def __init__(self, text, vector):\n",
    "        self.text = text\n",
    "        self.vector = vector\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, word_list):\n",
    "        self.word_list = word_list\n",
    "\n",
    "    def len(self) -> int:\n",
    "        return len(self.word_list)\n",
    "\n",
    "\n",
    "def get_word_frequency(word_text, looktable):\n",
    "    if word_text in looktable:\n",
    "        return looktable[word_text]\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "def sentence_to_vec(sentence_list: List[Sentence], embedding_size, looktable, a=1e-3):\n",
    "    sentence_set = []\n",
    "    for sentence in sentence_list:\n",
    "        vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
    "        sentence_length = sentence.len()\n",
    "        for word in sentence.word_list:\n",
    "            a_value = a / (a + get_word_frequency(word.text, looktable))  # smooth inverse frequency, SIF\n",
    "            vs = np.add(vs, np.multiply(a_value, word.vector))  # vs += sif * word_vector\n",
    "\n",
    "        vs = np.divide(vs, sentence_length)  # weighted average\n",
    "        sentence_set.append(vs)  # add to our existing re-calculated set of sentences\n",
    "\n",
    "    # calculate PCA of this sentence set\n",
    "    pca = PCA(n_components=embedding_size)\n",
    "    pca.fit(np.array(sentence_set))\n",
    "    u = pca.components_[0]  # the PCA vector\n",
    "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
    "\n",
    "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size - len(u)):\n",
    "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
    "\n",
    "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
    "    sentence_vecs = []\n",
    "    for vs in sentence_set:\n",
    "        sub = np.multiply(u, vs)\n",
    "        sentence_vecs.append(np.subtract(vs, sub))\n",
    "\n",
    "    return sentence_vecs\n",
    "\n",
    "\n",
    "with open(freq_table, 'rb') as f:\n",
    "    mydict = pkl.load(f)\n",
    "print('完成词频字典载入')\n",
    "\n",
    "paths = ['./datasets/data']\n",
    "for path in paths:\n",
    "    files = []\n",
    "    for file in os.listdir(path=path):\n",
    "        if os.path.isfile(path + '/' + file):\n",
    "            files.append(path + '/' + file)\n",
    "\n",
    "    for traindata in files:\n",
    "        with open(traindata, 'rb') as f:\n",
    "            train = pkl.load(f)\n",
    "\n",
    "        print('读取' + traindata + '数据完成')\n",
    "\n",
    "        gs = []\n",
    "        pred = []\n",
    "        allsent = []\n",
    "        for each in train:\n",
    "            # sent1, sent2, label = each.split('\\t')\n",
    "            if len(train[0]) == 3:\n",
    "                sent1, sent2, label = each\n",
    "            else:\n",
    "                sent1, sent2, label, _ = each\n",
    "            gs.append(float(label))\n",
    "            s1 = []\n",
    "            s2 = []\n",
    "            # sw1 = sent1.split()\n",
    "            # sw2 = sent2.split()\n",
    "            for word in sent1:\n",
    "                try:\n",
    "                    vec = model[word]\n",
    "                except KeyError:\n",
    "                    vec = np.zeros(embedding_size)\n",
    "                s1.append(Word(word, vec))\n",
    "            for word in sent2:\n",
    "                try:\n",
    "                    vec = model[word]\n",
    "                except KeyError:\n",
    "                    vec = np.zeros(embedding_size)\n",
    "                s2.append(Word(word, vec))\n",
    "\n",
    "            ss1 = Sentence(s1)\n",
    "            ss2 = Sentence(s2)\n",
    "            allsent.append(ss1)\n",
    "            allsent.append(ss2)\n",
    "\n",
    "        sentence_vectors = sentence_to_vec(allsent, embedding_size, looktable=mydict)\n",
    "        len_sentences = len(sentence_vectors)\n",
    "        for i in range(len_sentences):\n",
    "            if i % 2 == 0:\n",
    "                sim = cosine_similarity([sentence_vectors[i]], [sentence_vectors[i + 1]])\n",
    "                pred.append(sim[0][0])\n",
    "\n",
    "        print('len of pred: ', len(pred))\n",
    "        print('len of gs: ', len(gs))\n",
    "\n",
    "        r, p = pearsonr(pred, gs)\n",
    "        print(traindata + '皮尔逊相关系数:', r)\n",
    "\n",
    "\n",
    "        # sentence_vectors = sentence_to_vec([ss1, ss2], embedding_size, looktable=mydict)\n",
    "        # sim = cosine_similarity([sentence_vectors[0]], [sentence_vectors[1]])\n",
    "        # pred.append(sim[0][0])\n",
    "\n",
    "        # r, p = pearsonr(pred, gs)\n",
    "        # print(traindata + '皮尔逊相关系数:', r)  # print(sentence_vectors[0])\n",
    "# print(sentence_vectors[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

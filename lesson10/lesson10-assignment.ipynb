{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word2vec + fully connected neural networks to finish “⾖豆瓣评论” classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>脑子是个好东西，希望编剧们都能有。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        link name  \\\n",
       "0   1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1   2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2   3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3   4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4   5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "5   6  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "6   7  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "7   8  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "8   9  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "9  10  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  \n",
       "5                        “犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。    1  \n",
       "6                                  脑子是个好东西，希望编剧们都能有。    2  \n",
       "7  三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...    4  \n",
       "8  开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...    4  \n",
       "9  15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...    1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fname = 'movie_comments.csv'\n",
    "df = pd.read_csv(fname, encoding='utf8')\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261497, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>脑子是个好东西，希望编剧们都能有。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment star\n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1\n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2\n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2\n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4\n",
       "4                                               中二得很    1\n",
       "5                        “犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。    1\n",
       "6                                  脑子是个好东西，希望编剧们都能有。    2\n",
       "7  三星半，实打实的7分。第一集在爱国主旋律内部做着各种置换与较劲，但第二集才真正显露吴京的野心...    4\n",
       "8  开篇长镜头惊险大气引人入胜 结合了水平不俗的快剪下实打实的真刀真枪 让人不禁热血沸腾 特别弹...    4\n",
       "9  15/100吴京的冷峰在这部里即像成龙，又像杰森斯坦森，但体制外的同类型电影，主角总是代表个...    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只使用comment和star两列\n",
    "df = df[['comment', 'star']]\n",
    "# 删除comment为空的数据\n",
    "df = df[pd.notnull(df['comment'])]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261495, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新索引数据\n",
    "df.index = range(261495)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "\n",
    "def preprocess(comments):\n",
    "    pattern = re.compile('[\\u4E00-\\u9FA5]')\n",
    "    new_comments = []\n",
    "    for comment in comments:\n",
    "        only_chinese = pattern.findall(comment)\n",
    "        chinese_comment = ''.join(only_chinese)\n",
    "        chinese_tokens = jieba.cut(chinese_comment)\n",
    "        new_comments.append(' '.join(chinese_tokens))\n",
    "    return new_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DINGLI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.198 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "comments = preprocess(df['comment'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'吴京 意淫 到 了 脑残 的 地步 看 了 恶心 想 吐'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_star = df['star'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(count_star.index, count_star.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Star', fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## get sentence vector from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf8 -*-\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "import PSLvec as psl\n",
    "from nltk.tokenize import StanfordTokenizer\n",
    "\n",
    "word2vec_path = './GoogleNews-vectors-negative300.bin.gz'\n",
    "glove_path = './glove_model.txt'\n",
    "psl_path = './PSL_model.txt'\n",
    "# traindata = './datasets/sts2013.OnWN.pkl'\n",
    "freq_table = './mydictionary'\n",
    "embedding_size = 300\n",
    "\n",
    "pslemb = psl.PSL()\n",
    "\n",
    "# 载入word2vec模型\n",
    "# model = KeyedVectors.load_word2vec_format(word2vec_path,binary=True)\n",
    "# model = KeyedVectors.load_word2vec_format(glove_path,binary=False)\n",
    "# model = KeyedVectors.load_word2vec_format(psl_path,binary=False)\n",
    "model = pslemb.w\n",
    "print('完成模型载入')\n",
    "\n",
    "tokenizer = StanfordTokenizer(path_to_jar=r\"D:\\stanford-parser-full-2016-10-31\\stanford-parser.jar\")\n",
    "\n",
    "\n",
    "# print(type(model))\n",
    "# print(model['sdfsfsdfsadfs'])\n",
    "\n",
    "class Word:\n",
    "    def __init__(self, text, vector):\n",
    "        self.text = text\n",
    "        self.vector = vector\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, word_list):\n",
    "        self.word_list = word_list\n",
    "\n",
    "    def len(self) -> int:\n",
    "        return len(self.word_list)\n",
    "\n",
    "\n",
    "def get_word_frequency(word_text, looktable):\n",
    "    if word_text in looktable:\n",
    "        return looktable[word_text]\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "def sentence_to_vec(sentence_list: List[Sentence], embedding_size, looktable, a=1e-3):\n",
    "    sentence_set = []\n",
    "    for sentence in sentence_list:\n",
    "        vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
    "        sentence_length = sentence.len()\n",
    "        for word in sentence.word_list:\n",
    "            a_value = a / (a + get_word_frequency(word.text, looktable))  # smooth inverse frequency, SIF\n",
    "            vs = np.add(vs, np.multiply(a_value, word.vector))  # vs += sif * word_vector\n",
    "\n",
    "        vs = np.divide(vs, sentence_length)  # weighted average\n",
    "        sentence_set.append(vs)  # add to our existing re-calculated set of sentences\n",
    "\n",
    "    # calculate PCA of this sentence set\n",
    "    pca = PCA(n_components=embedding_size)\n",
    "    pca.fit(np.array(sentence_set))\n",
    "    u = pca.components_[0]  # the PCA vector\n",
    "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
    "\n",
    "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size - len(u)):\n",
    "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
    "\n",
    "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
    "    sentence_vecs = []\n",
    "    for vs in sentence_set:\n",
    "        sub = np.multiply(u, vs)\n",
    "        sentence_vecs.append(np.subtract(vs, sub))\n",
    "\n",
    "    return sentence_vecs\n",
    "\n",
    "\n",
    "with open(freq_table, 'rb') as f:\n",
    "    mydict = pkl.load(f)\n",
    "print('完成词频字典载入')\n",
    "\n",
    "paths = ['./datasets/data']\n",
    "for path in paths:\n",
    "    files = []\n",
    "    for file in os.listdir(path=path):\n",
    "        if os.path.isfile(path + '/' + file):\n",
    "            files.append(path + '/' + file)\n",
    "\n",
    "    for traindata in files:\n",
    "        with open(traindata, 'rb') as f:\n",
    "            train = pkl.load(f)\n",
    "\n",
    "        print('读取' + traindata + '数据完成')\n",
    "\n",
    "        gs = []\n",
    "        pred = []\n",
    "        allsent = []\n",
    "        for each in train:\n",
    "            # sent1, sent2, label = each.split('\\t')\n",
    "            if len(train[0]) == 3:\n",
    "                sent1, sent2, label = each\n",
    "            else:\n",
    "                sent1, sent2, label, _ = each\n",
    "            gs.append(float(label))\n",
    "            s1 = []\n",
    "            s2 = []\n",
    "            # sw1 = sent1.split()\n",
    "            # sw2 = sent2.split()\n",
    "            for word in sent1:\n",
    "                try:\n",
    "                    vec = model[word]\n",
    "                except KeyError:\n",
    "                    vec = np.zeros(embedding_size)\n",
    "                s1.append(Word(word, vec))\n",
    "            for word in sent2:\n",
    "                try:\n",
    "                    vec = model[word]\n",
    "                except KeyError:\n",
    "                    vec = np.zeros(embedding_size)\n",
    "                s2.append(Word(word, vec))\n",
    "\n",
    "            ss1 = Sentence(s1)\n",
    "            ss2 = Sentence(s2)\n",
    "            allsent.append(ss1)\n",
    "            allsent.append(ss2)\n",
    "\n",
    "        sentence_vectors = sentence_to_vec(allsent, embedding_size, looktable=mydict)\n",
    "        len_sentences = len(sentence_vectors)\n",
    "        for i in range(len_sentences):\n",
    "            if i % 2 == 0:\n",
    "                sim = cosine_similarity([sentence_vectors[i]], [sentence_vectors[i + 1]])\n",
    "                pred.append(sim[0][0])\n",
    "\n",
    "        print('len of pred: ', len(pred))\n",
    "        print('len of gs: ', len(gs))\n",
    "\n",
    "        r, p = pearsonr(pred, gs)\n",
    "        print(traindata + '皮尔逊相关系数:', r)\n",
    "\n",
    "\n",
    "        # sentence_vectors = sentence_to_vec([ss1, ss2], embedding_size, looktable=mydict)\n",
    "        # sim = cosine_similarity([sentence_vectors[0]], [sentence_vectors[1]])\n",
    "        # pred.append(sim[0][0])\n",
    "\n",
    "        # r, p = pearsonr(pred, gs)\n",
    "        # print(traindata + '皮尔逊相关系数:', r)  # print(sentence_vectors[0])\n",
    "# print(sentence_vectors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". train neaual network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes= 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training\n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for validation \n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    # Predictions for test\n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    \n",
    "    test_prediction =  tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {:.1f}\".format(accuracy(predictions, batch_labels)))\n",
    "            print(\"Validation accuracy: {:.1f}\".format(accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: {:.1f}\".format(accuracy(test_prediction.eval(), test_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

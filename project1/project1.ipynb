{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n dingliang python=3.6\n",
    "\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装\n",
    "\n",
    "```\n",
    "$ git clone https://github.com/HIT-SCIR/pyltp\n",
    "$ git submodule init\n",
    "$ git submodule update\n",
    "\n",
    "\n",
    "// for windows, edit these two file before build\n",
    "// D:\\works\\NLP\\kaikeba\\tools\\pyltp\\ltp\\src\\srl\\include\\extractor\\Converter.h, remove chinese\n",
    "// \n",
    "diff --git a/patch/libs/python/src/converter/builtin_converters.cpp b/patch/libs/python/src/converter/builtin_converters.cpp\n",
    "index 78e55fd..9873fe2 100644\n",
    "--- a/patch/libs/python/src/converter/builtin_converters.cpp\n",
    "+++ b/patch/libs/python/src/converter/builtin_converters.cpp\n",
    "@@ -48,7 +48,7 @@ namespace\n",
    " #else\n",
    "   void* convert_to_cstring(PyObject* obj)\n",
    "   {\n",
    "-      return PyUnicode_Check(obj) ? _PyUnicode_AsString(obj) : 0;\n",
    "+      return PyUnicode_Check(obj) ? (void *)_PyUnicode_AsString(obj) : 0;^M\n",
    "   }\n",
    " #endif\n",
    "\n",
    "$ python setup.py install\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 cut sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyltp.VectorOfString object at 0x000001303DB8EAB0>\n"
     ]
    }
   ],
   "source": [
    "from pyltp import SentenceSplitter\n",
    "document=u'昨日，雷先生说，交警部门罚了他16次，他只认了一次，交了一次罚款，拿到法院的判决书后，会前往交警队，要求撤销此前的处罚。'\n",
    "sentences = SentenceSplitter.split(document)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 cut word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昨日\t，\t雷\t先生\t说\t，\t交警\t部门\t罚\t了\t他\t16\t次\t，\t他\t只\t认\t了\t一\t次\t，\t交\t了\t一\t次\t罚款\t，\t拿\t到\t法院\t的\t判决书\t后\t，\t会\t前往\t交警队\t，\t要求\t撤销\t此前\t的\t处罚\t。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "LTP_DATA_DIR = r'../../tools/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "words = segmentor.segment(sentences[0])  # 分词\n",
    "print('\\t'.join(words))\n",
    "segmentor.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt\twp\tnh\tn\tv\twp\tj\tn\tv\tu\tr\tm\tq\twp\tr\td\tv\tu\tm\tq\twp\tv\tu\tm\tq\tv\twp\tv\tv\tn\tu\tn\tnd\twp\tv\tv\tn\twp\tv\tv\tnt\tu\tv\twp\n"
     ]
    }
   ],
   "source": [
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "postags = postagger.postag(words)  # 词性标注\n",
    "\n",
    "print('\\t'.join(postags))\n",
    "postagger.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\tO\tS-Nh\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n"
     ]
    }
   ],
   "source": [
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "\n",
    "from pyltp import NamedEntityRecognizer\n",
    "recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "recognizer.load(ner_model_path)  # 加载模型\n",
    "\n",
    "netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "\n",
    "print('\\t'.join(netags))\n",
    "recognizer.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:ADV\t1:WP\t4:ATT\t5:SBV\t0:HED\t5:WP\t8:ATT\t9:SBV\t5:VOB\t9:RAD\t9:IOB\t13:ATT\t9:VOB\t9:WP\t17:SBV\t17:ADV\t9:COO\t17:RAD\t20:ATT\t17:CMP\t17:WP\t17:COO\t22:RAD\t25:ATT\t26:ATT\t22:VOB\t22:WP\t33:ATT\t28:CMP\t32:ATT\t30:RAD\t28:VOB\t36:ADV\t33:WP\t36:ADV\t17:COO\t36:VOB\t17:WP\t17:COO\t39:VOB\t43:ATT\t41:RAD\t40:VOB\t5:WP\n"
     ]
    }
   ],
   "source": [
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "\n",
    "from pyltp import Parser\n",
    "parser = Parser() # 初始化实例\n",
    "parser.load(par_model_path)  # 加载模型\n",
    "\n",
    "arcs = parser.parse(words, postags)  # 句法分析\n",
    "\n",
    "print(\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "parser.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parse_child_dict(words, postags, arcs):\n",
    "    \"\"\"\n",
    "    为句子中的每个词语维护一个保存句法依存儿子节点的字典\n",
    "    Args:\n",
    "        words: 分词列表\n",
    "        postags: 词性列表\n",
    "        arcs: 句法依存列表\n",
    "    \"\"\"\n",
    "    child_dict_list = []\n",
    "    for index in range(len(words)):\n",
    "        child_dict = dict()\n",
    "        for arc_index in range(len(arcs)):\n",
    "            if arcs[arc_index].head == index + 1:\n",
    "                if arcs[arc_index].relation in child_dict:\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "                else:\n",
    "                    child_dict[arcs[arc_index].relation] = []\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "        #if child_dict.has_key('SBV'):\n",
    "        #    print words[index],child_dict['SBV']\n",
    "        child_dict_list.append(child_dict)\n",
    "    return child_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_dict_list = build_parse_child_dict(words, postags, arcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'WP': [1]}, {}, {}, {'ATT': [2]}, {'ADV': [0], 'SBV': [3], 'WP': [5, 43], 'VOB': [8]}, {}, {}, {'ATT': [6]}, {'SBV': [7], 'RAD': [9], 'IOB': [10], 'VOB': [12], 'WP': [13], 'COO': [16]}, {}, {}, {}, {'ATT': [11]}, {}, {}, {}, {'SBV': [14], 'ADV': [15], 'RAD': [17], 'CMP': [19], 'WP': [20, 37], 'COO': [21, 35, 38]}, {}, {}, {'ATT': [18]}, {}, {'RAD': [22], 'VOB': [25], 'WP': [26]}, {}, {}, {'ATT': [23]}, {'ATT': [24]}, {}, {'CMP': [28], 'VOB': [31]}, {}, {'RAD': [30]}, {}, {'ATT': [29]}, {'ATT': [27], 'WP': [33]}, {}, {}, {'ADV': [32, 34], 'VOB': [36]}, {}, {}, {'VOB': [39]}, {'VOB': [42]}, {'RAD': [41]}, {}, {'ATT': [40]}, {}]\n"
     ]
    }
   ],
   "source": [
    "print(child_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_e(words, postags, child_dict_list, word_index):\n",
    "    \"\"\"\n",
    "    完善识别的部分实体\n",
    "    \"\"\"\n",
    "    child_dict = child_dict_list[word_index]\n",
    "    prefix = ''\n",
    "    if 'ATT' in child_dict:\n",
    "        for i in range(len(child_dict['ATT'])):\n",
    "            prefix += complete_e(words, postags, child_dict_list, child_dict['ATT'][i])\n",
    "    \n",
    "    postfix = ''\n",
    "    if postags[word_index] == 'v':\n",
    "        if 'VOB' in child_dict:\n",
    "            postfix += complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "        if 'SBV' in child_dict:\n",
    "            prefix = complete_e(words, postags, child_dict_list, child_dict['SBV'][0]) + prefix\n",
    "\n",
    "    return prefix + words[word_index] + postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_triple_extract(words, postags, arcs):\n",
    "    child_dict_list = build_parse_child_dict(words, postags, arcs)\n",
    "    for index in range(len(postags)):\n",
    "        # 抽取以谓词为中心的事实三元组\n",
    "        if postags[index] == 'v':\n",
    "            child_dict = child_dict_list[index]\n",
    "            # 主谓宾\n",
    "            if ('SBV' in child_dict) and ('VOB' in child_dict):\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                r = words[index]\n",
    "                e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                print(\"主语谓语宾语关系\\t({}, {}, {})\\n\".format(e1, r, e2))\n",
    "                \n",
    "            # 定语后置，动宾关系\n",
    "            if arcs[index].relation == 'ATT':\n",
    "                if 'VOB' in child_dict:\n",
    "                    e1 = complete_e(words, postags, child_dict_list, arcs[index].head - 1)\n",
    "                    r = words[index]\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                    temp_string = r+e2\n",
    "                    if temp_string == e1[:len(temp_string)]:\n",
    "                        e1 = e1[len(temp_string):]\n",
    "                    if temp_string not in e1:\n",
    "                        print(\"定语后置动宾关系\\t({}, {}, {})\\n\".format(e1, r, e2))\n",
    "                        \n",
    "            # 含有介宾关系的主谓动补关系\n",
    "            if ('SBV' in child_dict) and ('CMP' in child_dict):\n",
    "                #e1 = words[child_dict['SBV'][0]]\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                cmp_index = child_dict['CMP'][0]\n",
    "                r = words[index] + words[cmp_index]\n",
    "                if 'POB' in child_dict_list[cmp_index]:\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict_list[cmp_index]['POB'][0])\n",
    "                    print(\"介宾关系主谓动补\\t({}, {}, {})\\n\".format(e1, r, e2))\n",
    "\n",
    "        # 尝试抽取命名实体有关的三元组\n",
    "        if netags[index][0] == 'S' or netags[index][0] == 'B':\n",
    "            ni = index\n",
    "            if netags[ni][0] == 'B':\n",
    "                while netags[ni][0] != 'E':\n",
    "                    ni += 1\n",
    "                e1 = ''.join(words[index:ni+1])\n",
    "            else:\n",
    "                e1 = words[ni]\n",
    "            if arcs[ni].relation == 'ATT' and postags[arcs[ni].head-1] == 'n' and netags[arcs[ni].head-1] == 'O':\n",
    "                r = complete_e(words, postags, child_dict_list, arcs[ni].head-1)\n",
    "                if e1 in r:\n",
    "                    r = r[(r.index(e1)+len(e1)):]\n",
    "                if arcs[arcs[ni].head-1].relation == 'ATT' and netags[arcs[arcs[ni].head-1].head-1] != 'O':\n",
    "                    e2 = complete_e(words, postags, child_dict_list, arcs[arcs[ni].head-1].head-1)\n",
    "                    mi = arcs[arcs[ni].head-1].head-1\n",
    "                    li = mi\n",
    "                    if netags[mi][0] == 'B':\n",
    "                        while netags[mi][0] != 'E':\n",
    "                            mi += 1\n",
    "                        e = ''.join(words[li+1:mi+1])\n",
    "                        e2 += e\n",
    "                    if r in e2:\n",
    "                        e2 = e2[(e2.index(r)+len(r)):]\n",
    "                    if r+e2 in sentence:\n",
    "                        print(\"人名//地名//机构\\t({}, {}, {})\\n\".format(e1, r, e2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主语谓语宾语关系\t(雷先生, 说, 交警部门罚16次)\n",
      "\n",
      "主语谓语宾语关系\t(交警部门, 罚, 16次)\n",
      "\n",
      "定语后置动宾关系\t(后, 拿, 法院判决书)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_triple_extract(words, postags, arcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_opinion(words, postags, arcs):\n",
    "    child_dict_list = build_parse_child_dict(words, postags, arcs)\n",
    "    index = 0\n",
    "    for arc in arcs:\n",
    "        if arc.relation == 'HED':\n",
    "            break\n",
    "        index+=1\n",
    "    \n",
    "    # 谓语是说一类的词\n",
    "    predicate = words[index]\n",
    "    child_dict = child_dict_list[index]\n",
    "    if ('SBV' in child_dict) and ('VOB' in child_dict):\n",
    "        e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "        r = words[index]\n",
    "        e2 = ''.join(words[index+1:])\n",
    "        print(\"主语谓语宾语关系\\t({}, {}, {})\\n\".format(e1, r, e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主语谓语宾语关系\t(雷先生, 说, ，交警部门罚了他16次，他只认了一次，交了一次罚款，拿到法院的判决书后，会前往交警队，要求撤销此前的处罚。)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_opinion(words, postags, arcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def token(string):\n",
    "    \"\"\"\n",
    "    only keep number and words\n",
    "    \"\"\"\n",
    "    return re.findall(r'[\\d|\\w]+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_documents():\n",
    "    csv_path = '../lesson05/sqlResult_1558435.csv'\n",
    "    database = pd.read_csv(csv_path, encoding='gb18030')\n",
    "    database = database.fillna('')\n",
    "    contents = database['content'].tolist()\n",
    "    contents = [n.replace('\\r\\n', '') for n in contents]\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/体验版内测，稳定版暂不受影响），以确保工程师可以集中全部精力进行系统优化工作。有人猜测这也是将精力主要用到MIUI 9的研发之中。MIUI 8去年5月发布，距今已有一年有余，也是时候更新换代了。当然，关于MIUI 9的确切信息，我们还是等待官方消息。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = get_documents()\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyltp import SentenceSplitter\n",
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "from pyltp import NamedEntityRecognizer\n",
    "from pyltp import Parser\n",
    "\n",
    "def build_parse_child_dict(words, postags, arcs):\n",
    "    \"\"\"\n",
    "    为句子中的每个词语维护一个保存句法依存儿子节点的字典\n",
    "    Args:\n",
    "        words: 分词列表\n",
    "        postags: 词性列表\n",
    "        arcs: 句法依存列表\n",
    "    \"\"\"\n",
    "    child_dict_list = []\n",
    "    for index in range(len(words)):\n",
    "        child_dict = dict()\n",
    "        for arc_index in range(len(arcs)):\n",
    "            if arcs[arc_index].head == index + 1:\n",
    "                if arcs[arc_index].relation in child_dict:\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "                else:\n",
    "                    child_dict[arcs[arc_index].relation] = []\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "        #if child_dict.has_key('SBV'):\n",
    "        #    print words[index],child_dict['SBV']\n",
    "        child_dict_list.append(child_dict)\n",
    "    return child_dict_list\n",
    "\n",
    "def complete_e(words, postags, child_dict_list, word_index):\n",
    "    \"\"\"\n",
    "    完善识别的部分实体\n",
    "    \"\"\"\n",
    "    child_dict = child_dict_list[word_index]\n",
    "    prefix = ''\n",
    "    if 'ATT' in child_dict:\n",
    "        for i in range(len(child_dict['ATT'])):\n",
    "            prefix += complete_e(words, postags, child_dict_list, child_dict['ATT'][i])\n",
    "\n",
    "    postfix = ''\n",
    "    if postags[word_index] == 'v':\n",
    "        if 'VOB' in child_dict:\n",
    "            postfix += complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "        if 'SBV' in child_dict:\n",
    "            prefix = complete_e(words, postags, child_dict_list, child_dict['SBV'][0]) + prefix\n",
    "\n",
    "    return prefix + words[word_index] + postfix\n",
    "\n",
    "def extract_opinion(document):\n",
    "    LTP_DATA_DIR = r'../../tools/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "    cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "    pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "    postagger = Postagger() # 初始化实例\n",
    "    postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "    ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "    recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "    recognizer.load(ner_model_path)  # 加载模型\n",
    "\n",
    "    par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "    parser = Parser() # 初始化实例\n",
    "    parser.load(par_model_path)  # 加载模型\n",
    "\n",
    "    # cut to sentences\n",
    "    sentences = SentenceSplitter.split(document)\n",
    "    table = []\n",
    "    for sentence in sentences:\n",
    "        print(sentence)\n",
    "        # cut words\n",
    "        words = segmentor.segment(sentence)  # 分词\n",
    "        print('\\t'.join(words))\n",
    "        # postages\n",
    "        postags = postagger.postag(words)  # 词性标注\n",
    "        print('\\t'.join(postags))\n",
    "        # ner\n",
    "        netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "        print('\\t'.join(netags))\n",
    "        # dependency parsing\n",
    "        arcs = parser.parse(words, postags)  # 句法分析\n",
    "        print(\"\\t\".join(\"{}:{}\".format(arc.head, arc.relation) for arc in arcs))\n",
    "\n",
    "        child_dict_list = build_parse_child_dict(words, postags, arcs)\n",
    "        index = 0\n",
    "        for arc in arcs:\n",
    "            if arc.relation == 'HED':\n",
    "                break\n",
    "            index+=1\n",
    "\n",
    "        # 谓语是说一类的词\n",
    "        predicate = words[index]\n",
    "        child_dict = child_dict_list[index]\n",
    "        if ('SBV' in child_dict) and ('VOB' in child_dict):\n",
    "            e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "            r = words[index]\n",
    "            e2 = ''.join(words[index+1:])\n",
    "            print(\"{} | {} | {}\".format(e1,r,e2))\n",
    "            table.append((e1,r,e2))\n",
    "\n",
    "\n",
    "    segmentor.release()  # 释放模型\n",
    "    postagger.release()  # 释放模型\n",
    "    recognizer.release()  # 释放模型\n",
    "    parser.release()  # 释放模型\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考虑性能而去屏蔽掉小核心。\n",
      "骁龙\t835\t作为\t唯一\t通过\tWindows\t10\t桌面\t平台\t认证\t的\tARM\t处理器\t，\t高通\t强调\t，\t不\t会\t因为\t只\t考虑\t性能\t而\t去\t屏蔽\t掉\t小\t核心\t。\n",
      "nz\tm\tp\tb\tp\tws\tm\tq\tn\tv\tu\tws\tn\twp\tnh\tv\twp\td\tv\tc\td\tv\tn\tc\tv\tv\tv\ta\tn\twp\n",
      "O\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tS-Nh\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "2:ATT\t16:ADV\t16:ADV\t16:ADV\t16:ADV\t9:ATT\t8:ATT\t9:ATT\t10:ATT\t13:ATT\t10:RAD\t13:ATT\t5:POB\t5:WP\t16:SBV\t0:HED\t16:WP\t19:ADV\t22:ADV\t22:ADV\t22:ADV\t16:VOB\t22:VOB\t26:ADV\t26:ADV\t22:COO\t26:CMP\t29:ATT\t26:VOB\t16:WP\n",
      "高通 | 强调 | ，不会因为只考虑性能而去屏蔽掉小核心。\n",
      "相反，他们正联手微软，找到一种适合桌面平台的、兼顾性能和功耗的完美方案。\n",
      "相反\t，\t他们\t正\t联手\t微软\t，\t找到\t一\t种\t适合\t桌面\t平台\t的\t、\t兼顾\t性能\t和\t功耗\t的\t完美\t方案\t。\n",
      "v\twp\tr\td\tv\tni\twp\tv\tm\tq\tv\tn\tn\tu\twp\tv\tn\tc\tn\tu\ta\tn\twp\n",
      "O\tO\tO\tO\tO\tS-Ni\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "5:ADV\t1:WP\t5:SBV\t5:ADV\t0:HED\t5:VOB\t5:WP\t5:COO\t10:ATT\t22:ATT\t22:ATT\t13:ATT\t11:VOB\t11:RAD\t16:WP\t11:COO\t16:VOB\t19:LAD\t17:COO\t11:RAD\t22:ATT\t8:VOB\t5:WP\n",
      "他们 | 联手 | 微软，找到一种适合桌面平台的、兼顾性能和功耗的完美方案。\n",
      "报道称，微软已经拿到了一些新的源码，以便Windows 10更好地理解big.little架构。\n",
      "报道\t称\t，\t微软\t已经\t拿\t到\t了\t一些\t新\t的\t源码\t，\t以便\tWindows\t10\t更\t好\t地\t理解\tbig.little\t架构\t。\n",
      "n\tv\twp\tni\td\tv\tv\tu\tm\ta\tu\tn\twp\td\tws\tm\td\ta\tu\tv\tws\tn\twp\n",
      "O\tO\tO\tS-Ni\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "2:SBV\t0:HED\t2:WP\t6:SBV\t6:ADV\t2:VOB\t6:CMP\t6:RAD\t12:ATT\t12:ATT\t10:RAD\t6:VOB\t6:WP\t20:ADV\t20:SBV\t20:ADV\t18:ADV\t20:ADV\t18:RAD\t6:COO\t22:ATT\t20:VOB\t2:WP\n",
      "报道 | 称 | ，微软已经拿到了一些新的源码，以便Windows10更好地理解big.little架构。\n",
      "资料显示，骁龙835作为一款集成了CPU、GPU、基带、蓝牙/Wi-Fi的SoC，比传统的Wintel方案可以节省至少30%的PCB空间。\n",
      "资料\t显示\t，\t骁龙\t835\t作为\t一\t款\t集成\t了\tCPU\t、\tGPU\t、\t基带\t、\t蓝牙/\tWi-Fi\t的\tSoC\t，\t比\t传统\t的\tWintel\t方案\t可以\t节省\t至少\t30%\t的\tPCB\t空间\t。\n",
      "n\tv\twp\tnz\tm\tv\tm\tq\tv\tu\tws\twp\tws\twp\tn\twp\twp\tws\tu\tws\twp\tp\tn\tu\tws\tn\tv\tv\td\tm\tu\tws\tn\twp\n",
      "O\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "2:SBV\t0:HED\t2:WP\t5:ATT\t6:SBV\t2:VOB\t8:ATT\t9:SBV\t6:VOB\t9:RAD\t9:VOB\t11:WP\t11:COO\t13:WP\t11:COO\t15:WP\t20:WP\t20:ATT\t18:RAD\t11:COO\t9:WP\t28:ADV\t26:ATT\t23:RAD\t26:ATT\t22:POB\t28:ADV\t9:COO\t30:ATT\t33:ATT\t30:RAD\t33:ATT\t28:VOB\t2:WP\n",
      "资料 | 显示 | ，骁龙835作为一款集成了CPU、GPU、基带、蓝牙/Wi-Fi的SoC，比传统的Wintel方案可以节省至少30%的PCB空间。\n",
      "按计划，今年Q4，华硕、惠普、联想将首发骁龙835 Win10电脑，预计均是二合一形态的产品。\n",
      "按\t计划\t，\t今年\tQ4\t，\t华硕\t、\t惠普\t、\t联想\t将\t首发\t骁龙\t835\tWin10\t电脑\t，\t预计\t均\t是\t二\t合\t一\t形态\t的\t产品\t。\n",
      "p\tn\twp\tnt\tws\twp\tnz\twp\tnz\twp\tnz\td\tv\tnz\tm\tws\tn\twp\tv\td\tv\tm\tv\tm\tn\tu\tn\twp\n",
      "O\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "13:ADV\t1:POB\t1:WP\t5:ATT\t13:SBV\t5:WP\t13:SBV\t7:WP\t7:COO\t9:WP\t7:COO\t13:ADV\t0:HED\t17:ATT\t17:ATT\t17:ATT\t13:VOB\t13:WP\t13:COO\t21:ADV\t19:VOB\t23:ADV\t21:VOB\t25:ATT\t27:ATT\t25:RAD\t23:VOB\t13:WP\n",
      "今年Q4 | 首发 | 骁龙835Win10电脑，预计均是二合一形态的产品。\n",
      "当然，高通骁龙只是个开始，未来也许还能见到三星Exynos、联发科、华为麒麟、小米澎湃等进入Windows 10桌面平台。\n",
      "当然\t，\t高通\t骁龙\t只\t是\t个\t开始\t，\t未来\t也许\t还\t能\t见到\t三星\tExynos\t、\t联发科\t、\t华为\t麒麟\t、\t小米\t澎湃\t等\t进入\tWindows\t10\t桌面\t平台\t。\n",
      "d\twp\tnz\tnz\td\tv\tq\tv\twp\tnt\td\td\tv\tv\tnz\tws\twp\tn\twp\tnz\tn\twp\tn\tv\tu\tv\tws\tm\tq\tn\twp\n",
      "O\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\tO\n",
      "6:ADV\t1:WP\t4:ATT\t6:SBV\t6:ADV\t0:HED\t8:ATT\t6:VOB\t6:WP\t14:ADV\t14:ADV\t13:ADV\t14:ADV\t6:COO\t16:ATT\t26:SBV\t16:WP\t16:COO\t18:WP\t21:ATT\t24:SBV\t21:WP\t21:COO\t26:SBV\t24:RAD\t14:VOB\t30:ATT\t29:ATT\t30:ATT\t26:VOB\t6:WP\n",
      "高通骁龙 | 是 | 个开始，未来也许还能见到三星Exynos、联发科、华为麒麟、小米澎湃等进入Windows10桌面平台。\n"
     ]
    }
   ],
   "source": [
    "table = extract_opinion(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
